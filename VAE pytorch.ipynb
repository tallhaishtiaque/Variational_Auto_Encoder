{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc368be-25f6-45b1-a87f-bac965e37e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 181.9058\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "latent_dim = 20\n",
    "\n",
    "# Load MNIST dataset \n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#note : transforms.Tensor ()convert the data from 0 to 1 range\n",
    "#       becasue of that we use the cross entropy loss\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1) # ouput size 14x14x 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # output size 7x7x64\n",
    "        \n",
    "        self.fc1 = nn.Linear(7  * 7 * 64, 256) #\n",
    "        self.fc2_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # view the tensor as single dimensional like flatten it\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        z_mean = self.fc2_mean(x)\n",
    "        z_logvar = self.fc2_logvar(x)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Define the decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 7 * 7 * 64)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = torch.relu(self.fc1(z))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = x.view(x.size(0), 64, 7, 7) # view the tensor as a single dimensional vector\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x)) # sigmoid output 786 value of range [0 to 1] for reconstruction image \n",
    "        return x\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) #=> logvar = log(std^2)=> output of variance vector\n",
    "                                      #=> std = exp(0.5 * logvar) => inorder to make it positve and negative values for a stable training. \n",
    "        eps = torch.randn_like(std) # Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encoder(x)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z_mean, z_logvar\n",
    "\n",
    "# Instantiate the VAE model and define the loss function and optimizer\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(x_recon, x, z_mean, z_logvar):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(x_recon, x, reduction='sum') # reduction = sum means output will be summed \n",
    "    kl_div = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Train the model\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, (images, _) in enumerate(train_loader):  # as we dont use label so put _ over there\n",
    "        # Forward pass\n",
    "        x = Variable(images) # Autograd automatically supports\n",
    "                             #Tensors with requires_grad set to True.\n",
    "                             #The original purpose of Variables was to be able to use automatic differentiation (Source):\n",
    "\n",
    "        x_recon, z_mean, z_logvar = model(x)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(x_recon, x, z_mean, z_logvar)\n",
    "        epoch_loss += loss.item()\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    train_loss.append(epoch_loss)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        x = Variable(images)\n",
    "        x_recon, z_mean, z_logvar = model(x)\n",
    "        loss = loss_fn(x_recon, x, z_mean, z_logvar)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Generate new images\n",
    "sample = Variable(torch.randn(64, latent_dim))\n",
    "sample = model.decoder(sample)\n",
    "sample = sample.view(64, 1, 28, 28).data\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(64):\n",
    "    plt.subplot(8, 8, i+1)\n",
    "    plt.imshow(sample[i][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011916f9-25ba-440d-af52-f3eac8f85d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
